---
title: "Rethinking Chapter 5"
author: "Gregor Mathes"
date: "2021-01-13"
slug: Rethinking Chapter 4
categories: []
tags: [Rethinking, Bayes, Statistics]
subtitle: ''
summary: 'This is the fourth part of a series where I work through the practice questions of the second edition of Richard McElreaths Statistical Rethinking'
authors: [Gregor Mathes]
lastmod: '2021-01-13T12:07:04+02:00'
featured: no
projects: [Rethinking]
output:
  blogdown::html_page:
    toc: true
    toc_depth: 1
    number_sections: true
    fig_width: 6
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.dim=c(7,4))
library(tidyverse)
library(rethinking)
```

# Introduction 

This is the fourth part of a series where I work through the practice questions of the second edition of Richard McElreaths [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/). Each post covers a new chapter and you can see the posts on previous chapters [here](https://gregor-mathes.netlify.app/tags/rethinking/).  
The third part of the series will cover chapter 5, which corresponds to the first part of week 3 of the lectures and homework (which you can find [here](https://github.com/rmcelreath/stat_rethinking_2020)). The homework of week 3 will be covered in the next part about chapter 6.   
From now on, I will set a given colour scheme for each chapter. This is mostly for me to see which colours play nice together, but will make the look of the blog posts more consistent.  
The colours for this blog post are: 

```{r colours}
red <- "#B74F35"
yellow <- "#FFB81C"
blue <- "#0E345E"
lighblue <- "#85ACA9"
```

  
# Easy practices   
  
## Question 5E1  
  
**Which of the linear models below are multiple linear regressions?**

(1) $$\mu_i = \alpha + \beta_xi$$
(2) $$\mu_i = \beta_x x_i + \beta_z z_i$$
(3) $$\mu_i = \alpha + \beta(x_i – z_i)$$
(4) $$\mu_i = \alpha + \beta_x x_i + \beta_z z_i$$

(1) contains only one predictor variable ($$\beta_xi$$) and is therefore a bivariate linear regression.  
(2) has two predictor variables and is a multiple linear regression without an intercept ($$\alpha$$).  
(3) the right side can written as $$\alpha + \beta x_i - \beta z_i$$ which looks like a weird multiple regression with negatively correlated slopes for each predictor.  
(4) is a perfectly looking multiple linear regression.
  
  
## Question 5E2  
  
**Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition.**  
  
Let $$\mu_i$$ be the mean animal diversity, **L** latitude, and **P** plant diversity.  
Then $$\mu_i = \alpha + \beta_L L_i + \beta_P P_i$$.   
  
  
## Question 5E3  
  
**Write down a multiple regression to evaluate the claim: Neither the amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on.**  
  
Let $$\mu_i$$ be the time to PhD, **F** the amount of funding, and **S** the size of laboratory.  
Then $$\mu_i = \alpha + \beta_F F_i + \beta_S S_i$$
Where both $$beta_F$$ & $$beta_S > 0$$  
  
  
## Question 5E4  
  
**Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C, and D. Let Ai be an indicator variable that is 1 where case i is in category A. Also suppose Bi, Ci, and Di for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it’s possible to compute one posterior distribution from the posterior distribution of another model.**  
  
(1) $$\mu_i = \alpha + \beta_A A_i + \beta_B B_i + \beta_D D_i$$
(2) $$\mu_i = \alpha + \beta_A A_i + \beta_B B_i + \beta_C C_i + \beta_D D_i$$
(3) $$\mu_i = \alpha + \beta_B B_i + \beta_C C_i + \beta_D D_i$$
(4) $$\mu_i = \alpha_A A_i + \alpha_B B_i + \alpha_C C_i + \alpha_D D_i$$
(5) $$\mu_i = \alpha_A (1 – B_i – C_i – D_i) + \alpha_B B_i + \alpha_C C_i + \alpha_D D_i$$  
  
This question was a bit to complicated for me and I just copied over the answer from [Jeffrey Girard]():  
  
*The first model includes a single intercept (for category C) and slopes for A, B, and D. The second model is non-identifiable because it includes a slope for all possible categories (page 156). The third model includes a single intercept (for category A) and slopes for B, C, and D. The fourth model uses the unique index approach to provide a separate intercept for each category (and no slopes). The fifth model uses the reparameterized approach on pages 154 and 155 to multiply the intercept for category A times 1 when in category A and times 0 otherwise. Models 1, 3, 4, and 5 are inferentially equivalent because they each allow the computation of each other’s posterior distribution (e.g., each category’s intercept and difference from each other category).*  
  
  
# Medium practices  
  
## Question 5M1  
  
**Invent your own example of a spurious correlation. An outcome variable should be correlated with both predictor variables. But when both predictors are entered in the same model, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).**  
  
Let's directly enter each simulation in a dataframe. For each variable, we sample 100 values from a normal distribution. The outcome variable is only related to the first predictor, but the second predictor is as well dependent on the first predictor. To make the selections of priors easier, I transform each variable into z-scores using the `scale()`.  
  
```{r 5M1 part 1}
N <- 100
dfr <- tibble(pred_1 = rnorm(N), 
       pred_2 = rnorm(N, -pred_1), 
       out_var = rnorm(N, pred_1)) %>% 
  mutate(across(everything(), scale))
```    
  
Now let's see how the outcome is related to the first predictor within a linear regression using quadratic approximation:  
Notice that I used priors that are not flat but instead are within a realistic realm. $$\alpha$$ must be pretty close to 0 when we standardise the outcome and the predictor. The prior on the slope $$\beta$$ is a bit more wider but still only captures realistic relationships as seen in prior predictive simulations throughout the chapter.  
  
```{r 5M1 part 2, warning=FALSE}
m1 <- alist(out_var ~ dnorm(mu, sigma),
      mu <- a + B1*pred_1,
      a ~ dnorm(0, 0.2), 
      B1 ~ dnorm(0, 0.5),
      sigma ~ dexp(1)) %>% 
  quap(., data = dfr) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate")
```  




--------------------------------------------------------------------------------
  
  
```{r session info}
sessionInfo()
```





