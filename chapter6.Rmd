---
title: "Rethinking Chapter 6"
author: "Gregor Mathes"
date: "2021-02-02"
slug: Rethinking Chapter 6
categories: []
tags: [Rethinking, Bayes, Statistics]
subtitle: ''
summary: 'This is the fifth part of a series where I work through the practice questions of the second edition of Richard McElreaths Statistical Rethinking'
authors: [Gregor Mathes]
lastmod: '2021-02-02T12:07:04+02:00'
featured: no
projects: [Rethinking]
output:
  html_document:
    toc: true
    toc_depth: 1
    number_sections: true
    fig_width: 6
    mathjax: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.dim=c(7,4), warning=FALSE, message = FALSE)
library(tidyverse)
library(rethinking)
library(ggdag)
library(dagitty)

map <- purrr::map
```

# Introduction  

This is the fifth part of a series where I work through the practice questions of the second edition of Richard McElreaths [Statistical Rethinking](%5Bhttps://xcelab.net/rm/statistical-rethinking/).](<https://xcelab.net/rm/statistical-rethinking/>).) Each post covers a new chapter and you can see the posts on previous chapters [here](%5Bhttps://gregor-mathes.netlify.app/tags/rethinking/).](<https://gregor-mathes.netlify.app/tags/rethinking/>).)

You can find the the lectures and homework accompanying the book [here](%5B%3Chttps://github.com/rmcelreath/stat_rethinking_2020%3E)).](<https://github.com/rmcelreath/stat_rethinking_2020>)).)

The colours for this blog post are:

```{r colour setup}
coral <- "#CD7672"
mint <- "#138086"
purple <- "#534666"
yellow <- "#EEb462"
```

```{r colour plot, echo=FALSE}
tibble(colours = c(coral, mint, purple, yellow), 
       colourname = c("coral", "mint", "purple", "yellow")) %>% 
  arrange(colours) %>% 
  mutate(colourname = fct_reorder(colourname, colours), 
         colourname = paste0(colours, "/ ", colourname)) %>% 
  ggplot() +
  geom_bar(aes(y = colours, fill = colours)) +
  scale_fill_identity() +
  geom_text(aes(x = 0.5, y = colours, label = colourname), 
            size = 6, colour = "white") +
  theme_void()
```  
  
The online version of the *Statistical Rethinking*, provided by the brilliant [Erik Kusch](https://www.erikkusch.com/), is missing a lot of practive questions, so I will focus on the examples from the print version here.  
  
# Easy practices  
  
## Question 6E1  
  
**List three mechanisms by which multiple regression can produce false inferences about causal effect.**  
  
The tree examples mentioned throughout the chapter were:
  
1. Collinearity  
2. Post-treatment bias  
3. Collider bias  
  
## Question 6E2  
  
**For one of the mechanisms in the previous problem, provide an example of your choice, perhaps from your own research.**  
  
- Collinearity  
If you want to estimate the effect of geographic range on the extinction risk organism in the fossil record, you can choose between a range of potential parameters that express geographic range. For example, you can use the convex hull area or the maximum pairwise great circle distance. However, if you add both parameters in a model their true magnitude of association to extinction
risk is lowered or even hidden, as they both encapsulate the same information.  
  
- Post-treatment bias  
Assume you want to estimate the effect of global mean temperature on the extinction risk of marine species in the fossil record. Additionally, you have an amazing data set on continental shelve area through time and would love to include that as well. However, temperature is quite likely causally related to shelve area as it drives eustatic sea level. So including shelve area in a model would shut the path between temperature and extinction risk, even though there is a real causal association.  
  
- Collider bias  
If anyone has a good example for a collider bias in palaeobiology, just message me on twitter (@GregorMathes).  
  
## Question 6E3  
  
**List the four elemental confounds. Can you explain the conditional dependencies of each?**  
  
1. Pipe  
  
```{r 6E3 part 1}
dagitty("dag{
        X -> Y -> Z}") %>% 
  impliedConditionalIndependencies()
```  
  
If we condition on `Y`, we shut the path between `X` and `Z`.  
  
2. Fork  
```{r 6E3 part 2}
dagitty("dag{
        X <- Y -> Z}") %>% 
  impliedConditionalIndependencies()
```  
  
If we condition on `Y`, then learning `X` tells us nothing about `Z`. All the information is in `Y`.

3. Collider  
  
```{r 6E3 part 3}
dagitty("dag{
        X -> Y <- Z}") %>% 
  impliedConditionalIndependencies()
```  
  
`X` is independent of `Z`. But conditioning on `Y` would open the path, and then `X`
would be dependent on `Z` conditional on `Y`.  
  
4. Descendant  
  
```{r 6E3 part 4}
dagitty("dag{
        X -> Y -> Z
        Y -> W}") %>% 
  impliedConditionalIndependencies()
```  
  
This is interesting. In the chapter, it says that if we would condition on `W`,
we would condition on `Y` as well (to a lesser extent). So I would have expected
X _||_ Z | W here.  
  
## Question 6E4  
  
**How is a biased sample like conditioning on a collider? Think of the example at the open of the chapter.**  
  
Assume the collider X -> Y <- Z.Conditioning on a collider `Y`  opens a path between `X` and `Z`, and leads to a spurious correlation between between these. This is similar to selection bias, where the researcher that sampled the data (or nature itself) cared about both `X` and `Z` when generating the sample.  
  
# Medium practices  
  
## Question 6M1  
  
**Modify the DAG on page 190 (page 186 in print) to include the variable V, an unobserved cause of C and Y: C <- V -> Y. Reanalyze the DAG. How many paths connect X to Y? Which must be closed? Which variables should you condition on now?**  
  
Let's update the DAG using the `ggdag` package:  
  
```{r 6M1 part 1}
tribble(
  ~ name,  ~ x,  ~ y,  
    "A",    1,     3,     
    "U",    0,     2,     
    "C",    2,     2,     
    "V",    3,     1,     
    "B",    1,     1,     
    "X",    0,     0,  
    "Y",    2,     0
) %>%  
  dagify(
    Y ~ X + C + V,
         C ~ V + A,
         B ~ C + U,
         U ~ A,
         X ~ U,
         coords = .) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(internal_colour = mint, alpha = 0.8, colour = "white") +
  geom_dag_text(aes(label = name), color = purple, size = 5) +
  geom_dag_edges(edge_color = purple) +
  labs(caption = "Figure 1: Adjusted DAG from the chapter, including V.") +
  theme_void()
```  
  
For the visual representations of the DAGs later on in this post, I will not show the code to generate the DAG plots to reduce the visual load on the reader. Anyways, you can look at the Rmd file with the raw code [here](https://github.com/Ischi94/statistical-rethinking/blob/master/chapter6.Rmd).  
  
So let's first identify each path from `X` to `Y`.  
(1) X -> Y  
(2) X <- U <- A -> C -> Y  
(3) X <- U <- A -> C <- V -> Y  
(4) X <- U -> B <- C -> Y  
(5) X <- U -> B <- C <- V -> Y  
  
C is now a collider in path (3) and the path is closed. Additionally, both paths passing through B are closed as well as `B` stays a collider. The only open backdoor path is (2), and to close this path we can condition on `A`.  
  
## Question 6M2  
  
**Sometimes, in order to avoid multicollinearity, people inspect pairwise correlations among predictors before including them in a model. This is a bad procedure, because what matters is the conditional association, not the association before the variables are included in the model. To highlight this, consider the DAG X -> Z -> Y. Simulate data from this DAG so that the correlation between X and Z is very large. Then include both in a model prediction Y. Do you observe any multicollinearity? Why or why not? What is different from the legs example in the chapter?**  
  
The simulation is rather simple. We let `Z` be completely dependent on `X` with just a little bit of noise, and render `Y` then completely dependent on `Z`. Note that I standardize each value to choose the priors more easily.  
  
```{r 6M2 part 1}
sim_dat <- tibble(x = rnorm(1e4),  
                  z = rnorm(1e4, x, 0.5), 
                  y = rnorm(1e4, z)) %>% 
  mutate(across(everything(), standardize))
```  
  
The correlation between `X` and `Z` is hence really large as all information flows through this path.  
  
```{r 6M2 part 2}
sim_dat %>% 
  summarise(correlation = cor(x, z))
```  
  
Now let's fit a model using quadratic approximation and plot the coefficients:  
  
```{r 6M2 part 3}
alist(y ~ dnorm(mu, sigma), 
      mu <- a + bx*x + bz*z, 
      a ~ dnorm(0, 0.2), 
      c(bx, bz) ~ dnorm(0, 0.5), 
      sigma ~ dexp(1)) %>% 
  quap(data = sim_dat) %>% 
  precis() %>% 
  as_tibble(rownames = "estimate") %>% 
  filter(estimate %in% c("bx", "bz")) %>% 
  rename(lower_pi = `5.5%`, upper_pi = `94.5%`) %>% 
  ggplot() +
  geom_linerange(aes(xmin = lower_pi, xmax = upper_pi, y = estimate), 
                 size = 1.5, colour = mint) +
  geom_point(aes(x = mean, y = estimate), 
             shape = 21, colour = "grey20", stroke = 1, 
             size = 5, fill = purple) +
  labs(y = NULL, x = "Estimate", caption = "Figure 2: Coefficient plot for simulated data with
       collinearity.") +
  theme_minimal()
```  
  
Not a big surprise, the effect of `X` is completely hidden as we condition on `Z` in a pipe. There is nothing new about `X` once the model knows `Z`. But to know that `Z` is only a mediatory variable, we need a causal model expressed in a DAG. The difference to the legs example here is that `X` and `Z` are causally related as `X` causes `Z`. It is therefore an example for a post-treatment bias. The leg lengths, on the other hand, where not causing each other, but were caused by a common parent instead:  
leg1 <- Parent -> leg2  
  
## Question 6M3  
  
**Learning to analyze DAGs requires practice. For each of the four DAGs below, state which variables, if any, you must adjust for (condition on) to estimate the total causal influence of X on Y.**  
  
```{r 6M3 part 1, echo=FALSE}
tribble(
  ~ name,  ~ x,  ~ y,  
  "A",    2,     1,     
  "Z",    1,     1,     
  "X",    0,     0,     
  "Y",    2,     0
) %>%  
  dagify(
    X ~ Z,
    Z ~ A, 
    Y ~ X + Z + A,
    coords = .) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(internal_colour = mint, alpha = 0.8, colour = "white") +
  geom_dag_text(aes(label = name), color = purple, size = 5) +
  geom_dag_edges(edge_color = purple) +
  labs(caption = "Figure 3: Directed acyclic graph number one.") +
  theme_void()
```

The are two backdoor paths, X <- Z -> Y and X <- Z <- A -> Y.  
Both are open and go through `Z`, so we can simply condition on `Z`.  
  
```{r 6M3 part 2, echo=FALSE}
tribble(
  ~ name,  ~ x,  ~ y,  
  "A",    2,     1,     
  "Z",    1,     1,     
  "X",    0,     0,     
  "Y",    2,     0
) %>%  
  dagify(
    Z ~ A + X, 
    Y ~ X + Z + A,
    coords = .) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(internal_colour = coral, alpha = 0.8, colour = "white") +
  geom_dag_text(aes(label = name), color = purple, size = 5) +
  geom_dag_edges(edge_color = purple) +
  labs(caption = "Figure 4: Directed acyclic graph number two.") +
  theme_void()
```  
  
There are two backdoor paths, X -> Z -> Y and X -> Z <- A -> Y.   
We want to keep X -> Z -> Y as it encapsulates information flowing from `X` to `Y` (and we
want to capture the total causal influence).   
The second path is closed as `Z` is a collider here.  
We don't need to adjust for anything.   
But note that conditioning on `Z` would open the second path and would be a mistake as it would create association between `X` on `A`.  
  
```{r 6M3 part 3, echo=FALSE}
tribble(
  ~ name,  ~ x,  ~ y,  
  "A",    0,     1,     
  "Z",    1,     1,     
  "X",    0,     0,     
  "Y",    2,     0
) %>%  
  dagify(
    X ~ A,
    Z ~ A + X + Y, 
    Y ~ X,
    coords = .) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(internal_colour = yellow, alpha = 0.8, colour = "white") +
  geom_dag_text(aes(label = name), color = purple, size = 5) +
  geom_dag_edges(edge_color = purple) +
  labs(caption = "Figure 5: Directed acyclic graph number three.") +
  theme_void()
```  
  
There are two backdoor paths: X <- A -> Z <- Y and X -> Z <- Y.  
Both paths are closed as `Z` is a collider for both.  
We don't need to condition on anything.   
Again, inluding `Z` in a model would create spurios relationships that we want to avoid.  
  
```{r 6M3 part 4, echo=FALSE}
tribble(
  ~ name,  ~ x,  ~ y,  
  "A",    0,     1,     
  "Z",    1,     1,     
  "X",    0,     0,     
  "Y",    2,     0
) %>%  
  dagify(
    X ~ A,
    Z ~ A + X, 
    Y ~ X + Z,
    coords = .) %>% 
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_node(colour = purple) +
  geom_dag_text(aes(label = name), color = "white", size = 5) +
  geom_dag_edges(edge_color = coral) +
  labs(caption = "Figure 5: Directed acyclic graph number four.") +
  theme_void()
```  
  
There are two backdoor paths: X <- A -> Z -> Y and X -> Z -> Y.   
We want to keep the second one but close the first one. For this, we can condition on `A`.   Conditioning on `Z` would close the first path as well, but also the second one which we want to keep as true causal.  
  



  









