---
title: "Rethinking Chapter 8"
author: "Gregor Mathes"
date: "2021-02-11"
slug: Rethinking Chapter 8
categories: []
tags: [Rethinking, Bayes, Statistics]
subtitle: ''
summary: 'Interaction terms as a modest introduction to mixed effect models'
authors: [Gregor Mathes]
lastmod: '2021-03-11T12:07:04+02:00'
featured: no
projects: [Rethinking]
output:
  html_document:
    toc: true
    toc_depth: 1
    number_sections: true
    fig_width: 6
    mathjax: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.dim=c(7,4), warning=FALSE, message = FALSE)
library(tidyverse)
library(rethinking)

map <- purrr::map
```

# Introduction

This is the seventh part of a series where I work through the practice questions of the second edition of Richard McElreaths [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/).\
Each post covers a new chapter and you can see the posts on previous chapters [here](https://gregor-mathes.netlify.app/tags/rethinking/). This chapter introduces linear interaction terms in regression models.

You can find the the lectures and homework accompanying the book [here](https://github.com/rmcelreath/stat_rethinking_2020%3E).

The colours for this blog post are:

```{r colour setup}
purple <- "#612D55"
lightpurple <- "#C7B8CC"
red <- "#C56A6A"
grey <- "#E4E3DD"
```

```{r colour plot, echo=FALSE}
tibble(colours = c(purple, lightpurple, red, grey), 
       colourname = c("purple", "lightpurple", "red", "grey")) %>% 
  arrange(colours) %>% 
  mutate(colourname = fct_reorder(colourname, colours), 
         colourname = paste0(colours, "/ ", colourname)) %>% 
  ggplot() +
  geom_bar(aes(y = colours, fill = colours)) +
  scale_fill_identity() +
  geom_text(aes(x = 0.5, y = colours, label = colourname), 
            size = 6, colour = "white") +
  theme_void()
```

I will not cover the homework from now on as it is mostly similar to the practice exercises. Further, I am a bit running out of time. Notice that the online version of the book is missing some of the practice exercises.

# Easy practices

## Question 8E1

**For each of the causal relationships below, name a hypothetical third variable that would lead to an interaction effect.**

- (1) Bread dough rises because of yeast

- (2) Education leads to higher income

- (3) Gasoline makes a car go

Learned this the hard way, but (1) really depends on temperature. Yeast needs to be added to the dough at the right temperature, otherwise your dough will stay flat and sad.

(2) I would say that motivation or drive plays into that as well. You will not get a good position or be successful in your job if you're a couch potato doing nothing all day, even if you have a very good education.  

And (3) can be dependent on so many variables. E.g., if you're car is broken, no amount of gasoline will make your car go.

## Question 8E2   

**Which of the following explanations invokes an interaction?**

- (1) Caramelizing onions requires cooking over low heat and making sure the onions do not dry out

- (2) A car will go faster when it has more cylinders or when it has a better fuel injector

- (3) Most people acquire their political beliefs from their parents, unless they get them instead from their friends

- (4) Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.)

Only (1) is a strict interaction, as the process of caramelizing onions is both dependent on low heat and not drying out. All the others are additive relationships.

## Question 8E3  

**For each of the explanations in 8E2, write a linear model that expresses the stated relationship**.

- (1) Let $u$ be the amount of caramelization, then $$mu_{i} = alpha + beta_{heat} * heat + beta_{dry} * dry + beta_{heatdry} * heat * dry$$  
  
- (2) Let $u$ be the speed of a car, then $$mu_{i} = alpha + beta_{cyl} * cyl + beta_{inj} * inj$$  
- (3) Let $u$ be the political belief, then $$mu_{i} = alpha + beta_{parent} * parent + beta_{friend} * friend$$  
  
- (4) Let $u$ be the intelligence, then $$mu_{i} = alpha + beta_{social} * social + beta_{append} * append$$  
  
  
# Medium practices  
  
## Question 8M1  
  
**Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of the water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature?**  
  
It seems like tulips don't bloom under higher temperatures, which creates a three-way interaction. Blooming is not only dependent on the interaction between water and shade, but this interaction depends on the temperature as well. If the temperature is too high, no amount of shade and water will make the tulip bloom.  
  
## Question 8M2  
  
**Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot?**  
  
If we code temperature as an index variable with a 0 for cold and a 1 for hot, we can multiply the whole initial model with $1 - temperature$. If the temperature is hot (= 0), the whole model will equate to zero bloom. Cold temperature, on the other hand has no effect on the bloom as the model just gets multiplied with 1.  
  
## Question 8M3  
  
**In parts of North America, ravens depend upon wolves for their food. This is because ravens are carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill and tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship is generally described as a “species interaction.” Can you invent a hypothetical set of data on raven population size in which this relationship would manifest as a statistical interaction? Do you think the biological interaction could be linear? Why or why not?**  
  
I will just simulate a simple wolf population using a poisson distribution with an average number of 10 wolfs, and then dependent on this population I will simulate the raven population.  
  
```{r 8M3 part 1, message=FALSE, fig.cap="Figure 1 | Species interaction between wolfes and ravens."}
tibble(wolf = rpois(1e3, lambda = 2), 
       raven = rpois(1e3, lambda = wolf + 2)) %>% 
  ggplot(aes(raven, wolf)) +
  geom_jitter(shape = 21, colour = purple, fill = grey, 
              size = 2, alpha = 0.8) +
  geom_smooth(colour = red, fill = lightpurple,
              alpha = 0.9) +
  scale_y_continuous(breaks = c(0, 5)) +
  labs(x = "Number of ravens", y = "Number of wolfes") +
  theme_minimal()
```  
  
Hypothetically, I would not expect a linear relationship. Instead I would predict an exponential relationship with the number of ravens *exploding* once wolfs are present.  
  
```{r 8M3 part 2, message=FALSE, fig.cap="Figure 2 | Improved species interaction between wolfes and ravens."}
tibble(wolf = rpois(1e3, lambda = 2), 
       raven = rnorm(1e3, mean = wolf)) %>% 
  filter(raven > 0) %>% 
  ggplot(aes(raven, wolf)) +
  geom_jitter(shape = 21, colour = purple, fill = grey, 
              size = 2, alpha = 0.8) +
  geom_smooth(colour = red, fill = lightpurple,
              alpha = 0.9) +
  scale_x_continuous(breaks = c(0, 5)) +
  labs(x = "Number of ravens", y = "Number of wolfes") +
  theme_minimal()
```  
  
## Question 8M4  
  
**Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior assumptions mean for the interaction prior, if anything?**  
  
Let's just update the model from the chapter with new priors, which we force to be positive by using a log-normal distribution. But first we load the data and bring it in a nicer format, centering `water` and `shade` and scaling `blooms` by its maximum.  
  
```{r 8M4 part 1}
data("tulips")

dat_tulips <- tulips %>% 
  as_tibble() %>% 
  mutate(water = water - mean(water), 
         shade = shade - mean(shade), 
         blooms = blooms/max(blooms)) 
```  
  
Now the model:  
  
```{r 8M4 part 2}
m1 <- alist(blooms ~ dnorm(mu, sigma), 
      mu <- a + bw*water + bs*shade + bws*water*shade, 
      a ~ dnorm(0.5, 0.25), 
      c(bw, bs, bws) ~ dlnorm(0, 0.25), 
      sigma ~ dexp(1)) %>% 
  quap(data = dat_tulips)
```  
  
Now we use `link()` for the prior predictive simulation, simulating ten lines.     
  
```{r 8M4 part 3, fig.cap="Figure 3 | Prior simulations for positive priors."}
N = 100
seq_dat <- tibble(water = rep(-1:1, N), 
                  shade = rep(-1:1, each = N))

extract.prior(m1, n = N) %>% 
  link(m1, post = ., data = seq_dat) %>% 
  as_tibble() %>% 
  pivot_longer(cols = everything(), values_to = "mu_blooms") %>%
  add_column(water = rep(seq_dat$water, N), 
             shade = rep(seq_dat$shade, N), 
             type = rep(as.character(1:N), each = length(seq_dat$water))) %>% 
  ggplot() +
  geom_line(aes(water, mu_blooms, group = type), 
            colour = lightpurple, alpha = 0.4) + 
  geom_point(aes(water, blooms), 
             shape = 21, colour = purple, fill = red, 
             size = 2, alpha = 0.8, 
             data = dat_tulips %>% filter(water %in% -1:1)) +
  facet_wrap(~shade) +
  theme_minimal()
```  
  
```{r 8M4 part 4, echo=FALSE}
plot_tryptich <- function(int.model) {
  extract.prior(int.model, n = N) %>% 
  link(int.model, post = ., data = seq_dat) %>% 
  as_tibble() %>% 
  pivot_longer(cols = everything(), values_to = "mu_blooms") %>%
  add_column(water = rep(seq_dat$water, N), 
             shade = rep(seq_dat$shade, N), 
             type = rep(as.character(1:N), each = length(seq_dat$water))) %>% 
  ggplot() +
  geom_line(aes(water, mu_blooms, group = type), 
            colour = lightpurple, alpha = 0.4) + 
  geom_point(aes(water, blooms), 
             shape = 21, colour = purple, fill = red, 
             size = 2, alpha = 0.8, 
             data = dat_tulips %>% filter(water %in% -1:1)) +
  facet_wrap(~shade) +
  theme_minimal()
}
```
  
Ehm, well. These are a bit too drastic and out of the realistic realm. It seems like we need to reduce the prior on the beta coefficients. For the log-normal distribution, we can do so by setting the meanlog to a negative number. The more negative the number, the closer we get to 0.  
  
```{r 8M4 part 5}
m2 <- alist(blooms ~ dnorm(mu, sigma), 
      mu <- a + bw*water + bs*shade + bws*water*shade, 
      a ~ dnorm(0.5, 0.25), 
      c(bw, bs, bws) ~ dlnorm(-2, 0.25), 
      sigma ~ dexp(1)) %>% 
  quap(data = dat_tulips)
```  
  
```{r 8M4 part 6, echo=FALSE, fig.cap="Figure 4 | Prior simulation with more reasonable priors."}
plot_tryptich(m2)
```  

That looks a lot more reasonable. What seems to be a bit off, though, is the direction of the relationship. We would actually expect a high and positive effect of water on blooms when the shade is low (= when there is a lot of light) and no effect when it's all shade (shade = 1). So actually the opposite of what we see in the priors. The reason for that is that we add a positive prior for shade in the model, resulting in a positive relationship. To get the relationship, we simply have to subtract it:  
  
```{r 8M4 part 7}
m3 <- alist(blooms ~ dnorm(mu, sigma), 
      mu <- a + bw*water - bs*shade + bws*water*shade, 
      a ~ dnorm(0.5, 0.25), 
      c(bw, bs, bws) ~ dlnorm(-2, 0.25), 
      sigma ~ dexp(1)) %>% 
  quap(data = dat_tulips)
```  
  
```{r 8M4 part 8, echo=FALSE, fig.cap="Figure 5 | Prior simulations forcing the relationship between blooms and shade negative."}
plot_tryptich(m3)
```  
  
Well this has changed nothing, because the interaction term is still added. We need to modify this as well.  
  
```{r 8M4 part 9}
m4 <- alist(blooms ~ dnorm(mu, sigma), 
      mu <- a + bw*water - bs*shade - bws*water*shade, 
      a ~ dnorm(0.5, 0.25), 
      c(bw, bs, bws) ~ dlnorm(-2, 0.25), 
      sigma ~ dexp(1)) %>% 
  quap(data = dat_tulips)
```  
  
```{r 8M4 part 10, echo=FALSE, fig.cap="Figure 6 | Prior simulations forcing the relationship between blooms and shade negative, as well as the relationship between blooms and the interaction of shade and water."}
plot_tryptich(m4)
```  
  
Yes, this is what we want. We encapsulate our knowledge in the priors, without rendering these priors too strong. They have enough variability so that the data can shine through them. They will improve the model performance however, as unrealistic values (flat priors) are not likely given the priors.  
  
  
# Hard practices  
  
## Question 8H1  
  
**Return to the data(tulips) example in the chapter. Now include the bed variable as a predictor in the interaction model. Don't interact bed with the other predictors; just include it as a main effect. Note that bed is categorical. So to use it properly, you will need to either construct dummy variables or rather an index variable, as explained in Chapter 6.**  
  
Recall the model m8.4 from the chapter:  
  
```{r 8H1 recall model}
m8.4 <- alist(blooms ~ dnorm(mu, sigma), 
      mu <- a + bw*water + bs*shade + bws*water*shade, 
      a ~ dnorm(0.5, 0.25), 
      c(bw, bs, bws) ~ dnorm(0, 0.25), 
      sigma ~ dexp(1)) %>%
  quap(data = dat_tulips)  
  
m8.4 %>% 
  precis() %>% 
  as_tibble(rownames = "estimate") %>% 
  knitr::kable(digits = 2)
```  
  
```{r 8H1 tidy_precis, echo=FALSE}
tidy_precis <- function(my_model = NULL){
  precis(my_model, depth = 2) %>% 
  as_tibble(rownames = "estimate") %>% 
  knitr::kable(digits = 2)
}
```


First, I transform `bed` to an integer to integrate it as an index variable.  
  
```{r 8H1 part 1}
dat_tulips <- dat_tulips %>% 
  mutate(bed = as.numeric(bed)) 
```  
  
Now let's add the `bed` variable to the model m8.4.    
  
```{r 8H1 part 2}
m1 <- alist(blooms ~ dnorm(mu, sigma), 
      mu <- a[bed] + bw*water + bs*shade + bws*water*shade, 
      a[bed] ~ dnorm(0.5, 0.25), 
      c(bw, bs, bws) ~ dnorm(0, 0.25), 
      sigma ~ dexp(1)) %>% 
  quap(data = dat_tulips)
```  
  
We can glimpse at the posterior distributions using a custom function build on precis (see the first code chunk from 8H1 for more details on `tidy_precis()`.  
  
```{r 8H1 part 3}
tidy_precis(m1)
```  
  
We can see that including `bed` in the model has not changed our inferences about the effect of `water`, `shade`, and their interaction on `bloom`. We can also see that there are differences between the beds. To quantify these differences, we need to calculate contrasts from the posterior.  
```{r 8H1 contrasts, fig.cap="Figure 7 | Contrast plot per bed."}
m1 %>% 
  extract.samples() %>% 
  pluck("a") %>% 
  as_tibble() %>% 
  transmute("bed1 - bed2" = V1 - V2, 
            "bed1 - bed3" = V1 - V3, 
            "bed2 - bed3" = V2 - V3) %>% 
  pivot_longer(cols = everything(), 
               values_to = "contrast", names_to = "contrast_type") %>% 
  ggplot(aes(contrast, fill = contrast_type, colour = contrast_type)) +
  geom_vline(xintercept = 0, colour = grey) +
  geom_density(alpha = 0.2) +
  scale_fill_manual(values = c(lightpurple, purple, red), 
                    name = NULL) +
  scale_colour_manual(values = c(lightpurple, purple, red), name = NULL) +
  labs(y = NULL, x = "Contrast") +
  theme_minimal() +
  theme(panel.grid = element_blank(), 
        axis.ticks = element_line(), 
        legend.position = c(0.9, 0.8))
```  
  
Now we can conclude that bed `bed2` and `bed3` have a substantially higher `bloom` than `bed1` (even though some area of the distribution tails extend above zero). There are no distinguishable differences between `bed2` and `bed3`.   
  
## Question 8H2  
  
**Use WAIC to compare the model from 8H1 to a model that omits bed. What do you infer from this comparison? Can you reconcile the WAIC results with the posterior distribution of the bed coefficients?**  
  
Let's just threw both models in the `compare()` function. 
  
```{r 8H2 waic}
compare(m8.4, m1) %>% 
  as_tibble(rownames = "model") %>% 
  knitr::kable(digits = 2)
```  
  
The model with beds included (`m1`) performs a little bit better. This is consistent with the posterior distributions of the bed coefficients, as we found some robust contrasts. This then can result in improved predictions and a better WAIC. However, the difference in WAIC between both models is very weak. It seems that the effect of bed is minor compared to all other predictors (`water` and `shade`).  
  
